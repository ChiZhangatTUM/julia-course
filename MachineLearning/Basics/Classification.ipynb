{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Classification with Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the reader to classification in [Flux](https://github.com/fluxml/flux.jl) using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits to test the classification ability of different networks. MNIST is made up of 60000 training images and 10000 testing images, with the lowest error rate ever achieved on the dataset being $0.23%$. Since 2017 an extended MNIST dataset (EMNIST) is available, but MNIST remains a benchmark for different approaches. The tutorial is based on amalgamation redux of two examples from the [Flux model zoo](https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl).\n",
    "\n",
    "Structure:\n",
    "    1. Classification using a multi-layer-perceptron\n",
    "    2. Classification using a convolutional neural network\n",
    "    3. Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification using a multi-layer-perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the MLP to go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "imgs = Flux.Data.MNIST.images()\n",
    "\n",
    "# Stack into one batch\n",
    "X = hcat(float.(reshape.(imgs, :))...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "labels = Flux.Data.MNIST.labels()\n",
    "\n",
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(labels, 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the MLP\n",
    "m = Chain(\n",
    "    Dense(28^2, 32, relu),\n",
    "    Dense(32, 10),\n",
    "    softmax\n",
    ")\n",
    "\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training\n",
    "dataset = repeated((X, Y), 200)\n",
    "evalcb = () -> @show(loss(X, Y))\n",
    "opt = ADAM()\n",
    "\n",
    "Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the accuracy\n",
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test set accuracy\n",
    "tX = hcat(float.(reshape.(Flux.Data.MNIST.images(:test), :))...)\n",
    "tY = onehotbatch(Flux.Data.MNIST.labels(:test), 0:9)\n",
    "\n",
    "accuracy(tX, tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification using a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and images\n",
    "train_labels = Flux.Data.MNIST.labels()\n",
    "train_imgs = Flux.Data.MNIST.images();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct minibatches\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "batch_size = 128\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n",
    "\n",
    "# Test set as one minibatch\n",
    "test_imgs = Flux.Data.MNIST.images(:test)\n",
    "test_labels = Flux.Data.MNIST.labels(:test)\n",
    "test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional architecture with three iterations of Conv -> ReLU -> MaxPool followed by\n",
    "# a final dense layer fed into a softmax probability output\n",
    "model = Chain(\n",
    "    # 1st convolutional layer, taking a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # 2nd convolutional layer, taking a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # 3rd convolutional layer, taking a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # Reshape 3d tensor into a 2d tensor of shape (3, 3, 32, N)\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "    \n",
    "    # Softmax output layer\n",
    "    softmax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional GPU training, if the data and model are sent to the available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is enabled uncomment the lines below to load the model onto a GPU\n",
    "#train_set = gpu.(train_set)\n",
    "#test_set = gpu.(test_set)\n",
    "#model = gpu(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile the model\n",
    "model(train_set[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossentropy loss between prediction and ground truth, add Gaussian noise to make model more robust\n",
    "function loss(x, y)\n",
    "    # Add random noise to x\n",
    "    x_aug = x .+ 0.1f0 * randn(eltype(x), size(x))\n",
    "    \n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ADAM as the optimizer\n",
    "opt = ADAM(0.001)\n",
    "\n",
    "best_acc = 0.0\n",
    "last_improvement = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop and train for 100 epochs\n",
    "for epoch_idx in 1:100\n",
    "    global best_acc, last_improvement\n",
    "    \n",
    "    # Train for one epoch\n",
    "    Flux.train!(loss, params(model), train_set, opt)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    acc = accuracy(test_set...)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "    \n",
    "    # Stop if accuracy is good enough\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    # Reduce learning rate if there has been no improvement for 5 epochs\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "        \n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "    \n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> The model has converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: Construct a different neural network for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment with other neural network architectures for classification:\n",
    "    - Construct a radial basis network and test it on MNIST\n",
    "         Hint: Combine it with Stheno to use Gaussians as special instances of radial basis functions as activations in a feed-forward network\n",
    "    - Construct an autoencoder and test its classification ability on MNIST\n",
    "         Hint: An [autoencoder](https://www.jeremyjordan.me/autoencoders/) consists on an encoder-decoder structure, which can be made up of only feed-forward, dense layers, or convolutional layers, which then amounts to a convolutional autoencoder.\n",
    "    - Challenge: Build a variational autoencoder and test its performance\n",
    "- Experiment with a mixture of the initial MLP-classification and convolutional classification, by introducing dense layers between convolutional layers.\n",
    "    - How does the training time change?\n",
    "    - How large is the influence of the activation functions on the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
