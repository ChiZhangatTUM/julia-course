{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Classification with Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the reader to classification in [Flux](https://github.com/fluxml/flux.jl) using the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits to test the classification ability of different networks. MNIST is made up of 60000 training images and 10000 testing images, with the lowest error rate ever achieved on the dataset being $0.23%$. Since 2017 an extended MNIST dataset (EMNIST) is available, but MNIST remains a benchmark for different approaches. The tutorial is based on amalgamation redux of two examples from the [Flux model zoo](https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl).\n",
    "\n",
    "Structure:\n",
    "    1. Classification using a multi-layer-perceptron\n",
    "    2. Classification using a convolutional neural network\n",
    "    3. Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification using a multi-layer-perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic neural network architecture to use is the multi-layer-perceptron also often called a feed-forward neural network. It has an input layer corresponding to the number of features we seek to feed to the neural network, and an output layer returning values, which can either be between $0$ and $1$ using `softmax` or capture nonlinearities inherent to the data we are dealing with by using rectified linear units `ReLU` or `tanh` activation functions.\n",
    "\n",
    "<img src=\"presentation/imgs/MLP.png\" width=\"600\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the images from MNIST and concatenating them into a single large vector `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "imgs = Flux.Data.MNIST.images()\n",
    "\n",
    "# Stack into one batch\n",
    "X = hcat(float.(reshape.(imgs, :))...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the labels and encode the labels using one-hot encoding, which in this corresponds to creating 10 columns, each representing a digit between 0-9, as can be found in MNIST, and then assigning each image a binary value. This allows us to encode the affiliation of every single image in a computer-friendly manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "labels = Flux.Data.MNIST.labels()\n",
    "\n",
    "# One-hot-encode the labels\n",
    "Y = onehotbatch(labels, 0:9);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Flux's `Chain` syntax to stack the individual layers, with `softmax` as output layer to return classification probabilities for each image. `crossentropy` then acts as a loss, utlizing a log-scale to penalize wrong classifications:\n",
    "\n",
    "<img src=\"presentation/imgs/cross_entropy.png\" width=\"450\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the MLP\n",
    "m = Chain(\n",
    "    Dense(28^2, 32, relu),\n",
    "    Dense(32, 10),\n",
    "    softmax\n",
    ")\n",
    "\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "accuracy(x, y) = mean(onecold(m(x)) .== onecold(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a data-iterator with `repeated` for training with 200 epochs, using the `ADAM` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training\n",
    "dataset = repeated((X, Y), 200)\n",
    "evalcb = () -> @show(loss(X, Y))\n",
    "opt = ADAM()\n",
    "\n",
    "Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the accuracy of the trained model on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the accuracy\n",
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the accuracy of the trained model on the previously unseen test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test set accuracy\n",
    "tX = hcat(float.(reshape.(Flux.Data.MNIST.images(:test), :))...)\n",
    "tY = onehotbatch(Flux.Data.MNIST.labels(:test), 0:9)\n",
    "\n",
    "accuracy(tX, tY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification using a convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative to MLPs is the use of convolutional neural networks, which through a series of kernel operations, pooling and fully-connected layers condense the input down to its essence and use that for classification/regression. An example CNN architecture for MNIST would look like this:\n",
    "\n",
    "<img src=\"presentation/imgs/CNN.png\" width=\"800\" height=\"530\" />\n",
    "\n",
    "Where we have one more convolutional layer, than in the image. A completely classical CNN architecture would furthermore look like this:\n",
    "\n",
    "$ Input \\rightarrow Conv \\rightarrow ReLU \\rightarrow Conv \\rightarrow ReLU \\rightarrow Pool \\rightarrow ReLU \\rightarrow Conv \\rightarrow ReLU \\rightarrow Pool \\rightarrow Fully Connected$\n",
    "\n",
    "The convolution-kernels in the first layer detect edges and curves, with subsequent layers further condensing information with activation maps representing more and more complex features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels and images\n",
    "train_labels = Flux.Data.MNIST.labels()\n",
    "train_imgs = Flux.Data.MNIST.images();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the images together with their respective labels and structure them as minibatches, which make them easiert to digest for our CPU/GPU. For larger models the size of the GPU memory governs the size of the batches. If you want to see the effect of minibatch size, change `batch_size` and then time the training with the `@tic` and `@toc` macros. The test set is structured as one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct minibatches\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "\n",
    "batch_size = 128\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs]\n",
    "\n",
    "# Test set as one minibatch\n",
    "test_imgs = Flux.Data.MNIST.images(:test)\n",
    "test_labels = Flux.Data.MNIST.labels(:test)\n",
    "test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a classical convolutional architecture with multiple $Conv \\rightarrow ReLU \\rightarrow MaxPool$ iterations followed by a final dense layer, which subsequently feeds its output into a softmax probability output, hence squashing the output values to probabilities on the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    # 1st convolutional layer, taking a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # 2nd convolutional layer, taking a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # 3rd convolutional layer, taking a 7x7 image\n",
    "    Conv((3, 3), 32=>32, pad=(1, 1), relu),\n",
    "    MaxPool((2, 2)),\n",
    "    \n",
    "    # Reshape 3d tensor into a 2d tensor of shape (3, 3, 32, N)\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "    \n",
    "    # Softmax output layer\n",
    "    softmax,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional GPU training, if the data and model are sent to the available GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is enabled uncomment the lines below to load the model onto a GPU\n",
    "#train_set = gpu.(train_set)\n",
    "#test_set = gpu.(test_set)\n",
    "#model = gpu(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precompile the model to gain a better speed advantage upon training, as the first compilation always takes the longest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_set[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again use crossentropy as the loss function, but do additionally inject Gaussian noise to make the process more noisy, which in turn makes the trained model more robust later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crossentropy loss between prediction and ground truth, add Gaussian noise to make model more robust\n",
    "function loss(x, y)\n",
    "    # Add random noise to x\n",
    "    x_aug = x .+ 0.1f0 * randn(eltype(x), size(x))\n",
    "    \n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the used ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = ADAM(0.001)\n",
    "\n",
    "best_acc = 0.0\n",
    "last_improvement = 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the training loop with included accuracy calculations, early stopping condition if the required accuracy is achieved and a reduction in learning rate if the optimizer becomes unable to minimize, i.e. overshoots the minimum as its learning rate is too large. One would usually consider to save the model parameters when the best model is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop and train for 100 epochs\n",
    "for epoch_idx in 1:100\n",
    "    global best_acc, last_improvement\n",
    "    \n",
    "    # Train for one epoch\n",
    "    Flux.train!(loss, params(model), train_set, opt)\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    acc = accuracy(test_set...)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "    \n",
    "    # Stop if accuracy is good enough\n",
    "    if acc >= 0.999\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of 99.9%\")\n",
    "        break\n",
    "    end\n",
    "    \n",
    "    # Reduce learning rate if there has been no improvement for 5 epochs\n",
    "    if epoch_idx - last_improvement >= 5 && opt.eta > 1e-6\n",
    "        opt.eta /= 10.0\n",
    "        @warn(\" -> Haven't improved in a while, dropping learning rate to $(opt.eta)!\")\n",
    "        \n",
    "        last_improvement = epoch_idx\n",
    "    end\n",
    "    \n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> The model has converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise: Construct a different neural network for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment with other neural network architectures for classification:\n",
    "    - Construct a radial basis network and test it on MNIST\n",
    "         Hint: Combine it with Stheno to use Gaussians as special instances of radial basis functions as activations in a feed-forward network\n",
    "    - Construct an autoencoder and test its classification ability on MNIST\n",
    "         Hint: An [autoencoder](https://www.jeremyjordan.me/autoencoders/) consists on an encoder-decoder structure, which can be made up of only feed-forward, dense layers, or convolutional layers, which then amounts to a convolutional autoencoder.\n",
    "    - Challenge: Build a variational autoencoder and test its performance\n",
    "- Experiment with a mixture of the initial MLP-classification and convolutional classification, by introducing dense layers between convolutional layers.\n",
    "    - How does the training time change?\n",
    "    - How large is the influence of the activation functions on the performance?\n",
    "- Construct the confusion matrix for the convolutional classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
