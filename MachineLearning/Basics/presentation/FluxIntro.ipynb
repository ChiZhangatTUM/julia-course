{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to Machine Learning with Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the reader to [Flux](https://github.com/fluxml/flux.jl) as a machine learning framework and its stylistic features and syntactical quirks. It is a condensed version of the 60 minute tutorial, which can be found in the [Flux model-zoo](https://github.com/FluxML/model-zoo/blob/master/tutorials/60-minute-blitz.jl) and lecture content created for the scientific machine learning course at TUM.\n",
    "\n",
    "## Outline\n",
    "\n",
    "**Section 1.** [Machine Learning Frameworks](#frameworks)\n",
    "\n",
    "**Section 2.** [Reminder - Arrays in Flux](#arrays)\n",
    "\n",
    "**Section 3.** [Sending Arrays to GPUs](#gpu)\n",
    "\n",
    "**Section 4.** [Automatic Differentiation](#autodiff)\n",
    "\n",
    "**Section 5.** [Example - Training a Classifier](#example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Metalhead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: Pkg not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Pkg not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[1]:1"
     ]
    }
   ],
   "source": [
    "Pkg.add(\"Tracker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda:\n",
    "\n",
    "- Quick overview over Machine learning frameworks in Julia\n",
    "- Introduction to Flux with an example\n",
    "    - Arrays\n",
    "    - Autodifferentiation\n",
    "    - Example: Let's build a classifier\n",
    "- Self-guided Tutorials\n",
    "    - Regression in Flux\n",
    "    - Classification in Flux\n",
    "    - Gaussian Processes using STheno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Machine Learning Frameworks <a name=\"frameworks\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the logic behind Julia's Machine learning ecosystem?\n",
    "\n",
    "- PyTorch and Tensorflow are both coping with the constraints of Python, while being their own constrained DSLs which are missing customization abilities, such as custom gradients\n",
    "- Explosion in the necessary computational power to achieve state-of-the-art results opens the door for high-performance\n",
    "- Julia is one of the languages, which is in a position to offer solutions to problems in expressiveness as well as being able to satisfy the needs for high-performance calculations of cutting edge research\n",
    "- View it inherently as a language-level problem\n",
    "- Computing AD-estimates in Julia allows gradients over all other Julia models, where others have to handle C++ models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Julia Ecosystem:\n",
    "\n",
    "- Flux (in tutorials throughout the day)\n",
    "- ONNX\n",
    "- Probabilistic Programming\n",
    "    - Gen (in tutorials later today)\n",
    "    - Turing (in tutorials later today)\n",
    "    - Stheno (in tutorials this morning)\n",
    "- Automatic Differentiation\n",
    "    - Zygote (used in some tutorials)\n",
    "    - Cassette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning:\n",
    "\n",
    "- Underdeveloped as of now\n",
    "- If you want the absolute best performance\n",
    "    - PyTorch natively written in C++\n",
    "    - CXX.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Reminder - Arrays in Flux <a name=\"arrays\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of Flux is comprised of its `arrays`, which in square form become matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = [1 2; 3 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For automatic generation of large arrays one relies on `rand`, which generates an array with random numbers between 0 and 1 in the shape $5 \\times 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication works with a Matlab-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = randn(5, 10)\n",
    "x = rand(10)\n",
    "W * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Sending Arrays to GPUs <a name=\"gpu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia can be fully integrated with NVIDIA GPUs with the `CUDAdrv`, `CUDAnative`, and `CuArrays` packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays\n",
    "\n",
    "N = 2^20\n",
    "\n",
    "x_d = CuArrays.fill(1.0f0, N)\n",
    "y_d = CuArrays.fill(2.0f0, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking the performance on addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime y_d .+= x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While CuArrays and the other CUDA-integrations produce fast code, we seek to stay on a higher abstraction level and do hence send our to the GPUs with the `cu` command, which can be as easy as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cu(rand(5, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to write your own GPU-kernels using `CUDAnative`, but the normal user will fare better by staying at a high abstraction level. But, as with the rest of Julia, the option is always available to dive deep into the code of the individual functions and write your own operations, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDAnative\n",
    "\n",
    "function gpu_add1!(y, x)\n",
    "    for i = 1:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@cuda gpu_add1!(y_d, x_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Automatic Differentiation <a name=\"autodiff\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second cornerstone of Flux is its automatic differentation ability, which seamlessly interfaces with the rest of the Julia ecosystem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"Automatic Differentiation (AD) is a set of techniques based on the mechanical application of the chain rule to obtain derivatives of a function given as a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations such as additions or elementary functions such as exp(). By applying the chain rule of derivative calculus repeatedly to these operations, derivatives of arbitrary order can be computed automatically, and accurate to working precision.\"*\n",
    "\n",
    "*Conceptually, AD is different from symbolic differentiation and approximations by divided differences.*\n",
    "\n",
    "(Source: [www.autodiff.org/?module=Introduction](http://www.autodiff.org/?module=Introduction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x^2 + 2x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Flux take the derivative for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tracker: gradient\n",
    "\n",
    "df(x) = gradient(f, x; nest=true)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking the derivative command down into we see that we have to feed the AD-engine the function `f`, the variable we want to take the derivative against `x`, whether the function is nested, and which derivative we seek to take. Any generic Julia code we write, can be taken the derivative of as long as the used mathematical functions are differentiable. Taking a Taylor approximation to the `sin` function for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysin(x) = sum((-1)^k * x^(1 + 2k)/factorial(1 + 2k) for k in 0:5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the AD-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.5\n",
    "\n",
    "val = gradient(mysin, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This machinery can also be applied to `arrays`; testing with a custom loss function we obtain gradients for each of the three inputs `W`, `b`, and `x`. Gradients, which can be utilized in classical optimization, as well as in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myloss(W, b, x) = sum(W * x .+ b)\n",
    "\n",
    "W = randn(3, 5)\n",
    "b = zeros(3)\n",
    "x = rand(5)\n",
    "\n",
    "gradient(myloss, W, b, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way to obtain gradients in Flux is by marking arrays with `param`, hence telling Flux to trace the execution of these arrays for later derivation. This branch has sadly been deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tracker: param, back!, grad\n",
    "\n",
    "W = param(randn(3, 5))\n",
    "b = param(zeros(3))\n",
    "x = rand(5)\n",
    "\n",
    "y = sum(W * x .+ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this branch is deprecated it still serves as a good illustration of gradient descent's approach. Update the weights and perform optimisation of the weights using gradient descent, the formula for which is\n",
    "\n",
    "$weights = weights - learning\\_rate * gradient$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Tracker: update!\n",
    "\n",
    "η = 0.1\n",
    "for p in params(m)\n",
    "    update!(p, -η * grad(p))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with optimisers inherent to Flux we only have to define the learning for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Descent(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now only need to construct a data iterator, which includes the number of epochs and is then feed to Flux's `train!` function for the actual training process. An alternative to this approach is to loop over our dataset and update our parameters in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = rand(10, 100), fill(0.5, 2, 100)\n",
    "loss(x, y) = sum(Flux.crossentropy(m(x), y))\n",
    "Flux.train!(loss, params(m), [(data, labels)], opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Example - Training a Classifier <a name=\"example\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Metalhead.jl` as a framework for computer vision models packaged with predefined & pretrained models and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Flux, Flux.Optimise\n",
    "using Metalhead, Images\n",
    "using Metalhead: trainimgs\n",
    "using Images.ImageCore\n",
    "using Flux: onehotbatch, onecold\n",
    "using Base.Iterators: partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the CIFAR10 dataset we are faced with the following classification task:\n",
    "\n",
    "\n",
    "<img src=\"imgs/cifar10.png\" width=\"600\" height=\"300\" />\n",
    "\n",
    "\n",
    "We begin by one-hot encoding the different classes, i.e. split up the association with certain classes in different categories to achieves binary labels, which are easier for the model to digest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metalhead.download(CIFAR10)\n",
    "X = trainimgs(CIFAR10)\n",
    "labels = onehotbatch([X[i].ground_truth.class for i in 1:50000], 1:10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to access images from the data set and assess the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image(x) = x.img\n",
    "ground_truth(x) = x.ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images of the cifar10 dataset can be viewed as $32 \\times 32$ matrices with 3 color-channels. The images are then to rearranged into batches and a validation set in preparation for the minibatch learning. Minibatch learning is supposed to prevent us from getting stuck in saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getarray(X) = float.(permutedims(channelview(X), (2, 3, 1)))\n",
    "imgs = [getarray(X[i].img) for i in 1:50000];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into a training and validation dataset with $98\\%$ reserved for training and $2\\%$ left for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gpu.([(cat(imgs[i]..., dims=4), labels[:, i]) for i in partition(1:49000, 1000)])\n",
    "valset = 49001:50000\n",
    "valX = cat(imgs[valset]..., dims=4) |> gpu\n",
    "valY = labels[:, valset] |> gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network is able to digest features of images it analyzes by sliding its convolution kernel over the matrix to then return an intermediate representation, which unearth successively higher-order features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Chain(\n",
    "    Conv((5, 5), 3=>16, relu),\n",
    "    MaxPool((2, 2)),\n",
    "    Conv((5, 5), 16=>8, relu),\n",
    "    MaxPool((2, 2)),\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(200, 120),\n",
    "    Dense(120, 84),\n",
    "    Dense(84, 10),\n",
    "    softmax) |> gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rely on the cross-entropy loss to adequately penalize for misclassifications with its log-skewing of the loss:\n",
    "\n",
    "<img src=\"imgs/cross_entropy.png\" width=\"450\" height=\"450\" />\n",
    "\n",
    "This is used in conjunction with the ADAM optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: crossentropy, Momentum\n",
    "\n",
    "loss(x, y) = sum(crossentropy(m(x), y))\n",
    "opt = ADAM(0.001);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep tab of the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(x, y) = mean(onecold(m(x), 1:10) .== onecold(y, 1:10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the network for 10 epochs of optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Callback function\n",
    "evalcb = () -> @show(accuracy(valX, valY))\n",
    "\n",
    "for epich = 1:epochs\n",
    "    Flux.train!(loss, params(m), train, opt, cb=evalcb)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the class labels of the neural network's outputs, and compare it to the groud-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of the validation data set\n",
    "valset = valimgs(CIFAR10)\n",
    "valimg = [getarray(valset[i].img) for i in 1:10000]\n",
    "labels = onehotbatch([valset[i].ground_truth.class for i in 1:10000],1:10)\n",
    "test = gpu.([(cat(valimg[i]..., dims = 4), labels[:,i]) for i in partition(1:10000, 1000)])\n",
    "\n",
    "ids = rand(1:10000, 10)\n",
    "image.(valset[ids])\n",
    "\n",
    "# Assess the model's performance\n",
    "rand_test = getarray.(image.(valset[ids]))\n",
    "rand_test = cat(rand_test..., dims = 4) |> gpu\n",
    "rand_truth = ground_truth.(valset[ids])\n",
    "m(rand_test)\n",
    "\n",
    "# Check the model's performance on test data\n",
    "accuracy(test[1]...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the classifier's performance on all classes on their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = zeros(10)\n",
    "class_total = zeros(10)\n",
    "for i in 1:10\n",
    "    preds = m(test[i][1])\n",
    "    lab = test[i][2]\n",
    "    for j = 1:1000\n",
    "        pred_class = findmax(preds[:, j])[2]\n",
    "        actual_class = findmax(lab[:, j])[2]\n",
    "        if pred_class == actual_class\n",
    "            class_correct[pred_class] += 1\n",
    "        end\n",
    "        class_total[actual_class] += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "class_correct ./ class_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
