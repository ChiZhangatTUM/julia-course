{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Gaussian Processes in Julia with Stheno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial gives an introduction to Gaussian processes and their implementation in the [STheno](https://github.com/willtebbutt/Stheno.jl) package in the Julia ecosystem. The tutorial is based on a selected amalgamation of the STheno [tutorial](https://willtebbutt.github.io/Stheno.jl/stable/getting_started/), questions one usually has to figure out when seeking to employ Gaussian processes in research or in production and a selected step-by-step [example](https://github.com/willtebbutt/stheno_models/blob/master/exact/simple_sensor_fusion.jl).\n",
    "\n",
    "Structure:\n",
    "    1. Construction of a first Gaussian process\n",
    "    2. Fitting a GP with BFGS\n",
    "    3. Approximate Bayesian inference of the GPs hyperparameters\n",
    "    4. Exercise\n",
    "    5. Sensor fusion with Stheno\n",
    "    6. Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Stheno, Plots, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random, Statistics, Orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Stheno: @model, EQ, Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construct a first Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the length-scale and variance of the process\n",
    "l = 0.4\n",
    "σ² = 1.3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a kernel from the available kernels in Stheno and feed the kernel to the gaussian process, where `GPC` is a mutable structure containing the collection of GPs and is used to keep track of the GP constructed in this case. It is comparable to a `trace` in a probabilistic program; a structure to store all results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a kernel\n",
    "k = σ² * stretch(matern52(), 1 / l)\n",
    "\n",
    "# Construct a zero-mean GP with the kernel\n",
    "f = GP(k, GPC());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an initial sample from the GP we previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample from the GP\n",
    "const x = randn(100)\n",
    "σ²_n = 0.05\n",
    "fx = f(x, σ²_n)\n",
    "const y = rand(fx);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the logarithmic probability between `f` at `x` (fx) and `y`, where the logarithmic probability is used as it is easier, and more accurate to calculate for a computer and the negative log probability represents the information content of our measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log marginal likelihood for this observation\n",
    "logpdf(fx, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "plt = plot();\n",
    "scatter!(plt, x, y; color=:red, label=\"\");\n",
    "display(plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the posterior over f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_posterior = f | Obs(fx, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the fit of our gaussian process to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the posterior\n",
    "x_plot = range(-4.0, 4.0; length=1000);\n",
    "plot!(plt, f_posterior(x_plot); samples=10, label=\"\", color=:blue);\n",
    "display(plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GP fitting with BFGS using Zygote.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BFGS optimiztion algorithm usually delivers the fastest convergence for smaller-sized Gaussian processes. It requires access to first-order gradients, which is why use Zygote's reverse-mode algorithmic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote: gradient\n",
    "\n",
    "# Set up the optimization\n",
    "θ0 = randn(3)\n",
    "results = Optim.optimize(nlml, θ->gradient(nlml, θ)[1], θ0, BFGS(); inplace=false)\n",
    "σ²_bfgs, l_bfgs, σ²_n_bfgs = unpack(results.minimizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = σ²_bfgs * stretch(matern52(), 1 / l_bfgs);\n",
    "f = GP(k, GPC());\n",
    "f_posterior_bfgs = f | Obs(f(x, σ²_n_bfgs), y);\n",
    "plot!(plt, f_posterior_bfgs(x_plot); samples=10, color=:purple, label=\"\");\n",
    "display(plt);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approximate Bayesian inference of the GP's hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use AdvancedHMC, for an optimized version of the No-U-Turns sampler, which relies on Zygote to obtain gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AdvancedHMC, Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model we want to sample from including gradients to accelerate convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log marginal likelihood function and its gradient\n",
    "lπ(θ) = -nlml(θ)\n",
    "function dlπdθ(θ)\n",
    "    lml, back = Zygote.pullback(lπ, θ)\n",
    "    dθ = first(back(1.0))\n",
    "    return lml, dθ\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of samples and the number of adaptations, if you have patience you can loop over different numbers of samples to get a convergence plot of the inference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling parameter settings\n",
    "n_samples, n_adapts = 100, 20\n",
    "\n",
    "# Draw random starting points\n",
    "θ0 = randn(3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the inference algorithm by first constructing the Hamiltonian for our model problem and then calibrating the No-U-Turn sampler and the leapfrogging algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric space, Hamiltonian, sampling method and adaptor\n",
    "metric = DiagEuclideanMetric(3)\n",
    "h = Hamiltonian(metric, lπ, dlπdθ)\n",
    "int = Leapfrog(find_good_eps(h, θ0))\n",
    "prop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)\n",
    "adaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the constructed Hamiltonian 100 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "samples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the posterior by plotting the convergence of its performance parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect posterior distribution\n",
    "hypers = unpack.(samples);\n",
    "plt_hypers = plot();\n",
    "plot!(plt_hypers, getindex.(hypers, 1); label=\"variance\");\n",
    "plot!(plt_hypers, getindex.(hypers, 2); label=\"length scale\");\n",
    "plot!(plt_hypers, getindex.(hypers, 3); label=\"obs noise variance\");\n",
    "display(plt_hypers);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment with different available kernels, Stheno offers a number of different [kernels](https://github.com/willtebbutt/Stheno.jl/blob/master/src/gp/kernel.jl), such as:\n",
    "    - Exponential kernel\n",
    "    - Exponentiated quadratic kernel\n",
    "    - Matern 32 kernel\n",
    "    - Matern 52 kernel\n",
    "    - Rational quadratic kernel `RQ(alpha)`, with kurtosis `alpha`\n",
    "    - Cosine kernel `Cosine(Tp)`, with period parameter `p`\n",
    "    - Gamma-Exponential kernel `GammaExp(Tgamma)`, where $0 < \\gamma \\leq 2$\n",
    "    - ..\n",
    "- How do the different kernels affect the posterior in an approximate Bayesian inference setting?\n",
    "- How does the number of samples, needed for an expressive posterior, change with the kernel choice?\n",
    "- Redefine nlml to make it a probabilistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example: Sensor fusion with STheno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assume `f` to be a real-valued function whose form and parameterization is unknown to us, which we hence seek to infer from data. This is an inverse problem for which Stheno and probabilistic programming are especially suited.\n",
    "\n",
    "Here we assume to have two sensor, whose measurements we are able to access. For the first sensor's measurements we assume:\n",
    "$\\mu = \\sin(x) - 5 + \\sqrt{abs(x)}$\n",
    "$\\sigma^{2} = 1e-2$\n",
    "\n",
    "And for the second sensor we assume to have a bias of `3.5`. From this we can construct our Bayesian model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function model()\n",
    "    \n",
    "    # We wish to infer a smooth latent process\n",
    "    f = GP(EQ())\n",
    "    \n",
    "    # Define the two noisy sensor measurements described above\n",
    "    noise1 = sqrt(1e-2) * GP(Noise()) + (x->sin.(x) .- 5.0 .+ sqrt.(abs.(x)))\n",
    "    noise2 = sqrt(1e-1) * GP(3.5, Noise())\n",
    "    \n",
    "    # Define the observation processes\n",
    "    y1 = f + noise1\n",
    "    y2 = f + noise2\n",
    "    \n",
    "    return f, noise1, noise2, y1, y2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate variables with initial model measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, noise₁, noise₂, y₁, y₂ = model();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy dataset to test the validity of our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = MersenneTwister(123456)\n",
    "\n",
    "X₁, X₂ = sort(rand(rng, 3) * 10), sort(rand(rng, 10) * 10);\n",
    "ŷ₁, ŷ₂ = rand(rng, [y₁(X₁), y₂(X₂)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(f′, y₁′, y₂′) = (f, y₁, y₂) | (y₁(X₁)←ŷ₁, y₂(X₂)←ŷ₂);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the posterior processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = range(-2.5, stop=12.5, length=500);\n",
    "f′Xp, y₁′Xp, y₂′Xp = rand(rng, [f′(Xp, 1e-9), y₁′(Xp, 1e-9), y₂′(Xp, 1e-9)], 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the posterior marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = marginals(f′(Xp));\n",
    "ms2 = marginals(y₁′(Xp));\n",
    "ms3 = marginals(y₂′(Xp));\n",
    "\n",
    "μf′, σf′ = mean.(ms1), std.(ms1);\n",
    "μy₁′, σy₁′ = mean.(ms2), std.(ms2);\n",
    "μy₂′, σy₂′ = mean.(ms3), std.(ms3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly();\n",
    "\n",
    "posterior_plot = plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the posterior marginal standard deviation to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot!(posterior_plot, Xp, [μy₁′ μy₁′];\n",
    "    linewidth=0.0,\n",
    "    fillrange=[μy₁′ .- 3 .* σy₁′, μy₁′ .+ 3 * σy₁′],\n",
    "    fillalpha=0.3,\n",
    "    fillcolor=:red,\n",
    "    label=\"\");\n",
    "plot!(posterior_plot, Xp, [μy₂′ μy₂′];\n",
    "    linewidth=0.0,\n",
    "    fillrange=[μy₂′ .- 3 .* σy₂′, μy₂′ .+ 3 * σy₂′],\n",
    "    fillalpha=0.3,\n",
    "    fillcolor=:green,\n",
    "    label=\"\");\n",
    "plot!(posterior_plot, Xp, [μf′ μf′];\n",
    "    linewidth=0.0,\n",
    "    fillrange=[μf′.- 3  .* σf′ μf′ .+ 3 .* σf′],\n",
    "    fillalpha=0.5,\n",
    "    fillcolor=:blue,\n",
    "    label=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the posterior marginal samples to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter!(posterior_plot, Xp, y₁′Xp,\n",
    "    markercolor=:red,\n",
    "    markershape=:circle,\n",
    "    markerstrokewidth=0.0,\n",
    "    markersize=0.5,\n",
    "    markeralpha=0.3,\n",
    "    label=\"\");\n",
    "scatter!(posterior_plot, Xp, y₂′Xp,\n",
    "    markercolor=:green,\n",
    "    markershape=:circle,\n",
    "    markerstrokewidth=0.0,\n",
    "    markersize=0.5,\n",
    "    markeralpha=0.3,\n",
    "    label=\"\");\n",
    "plot!(posterior_plot, Xp, f′Xp;\n",
    "    linecolor=:blue,\n",
    "    linealpha=0.2,\n",
    "    label=\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the posterior mean to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot!(posterior_plot, Xp, μy₁′;\n",
    "    linecolor=:red,\n",
    "    linewidth=2.0,\n",
    "    label=\"\");\n",
    "plot!(posterior_plot, Xp, μy₂′;\n",
    "    linecolor=:green,\n",
    "    linewidth=2.0,\n",
    "    label=\"\");\n",
    "plot!(posterior_plot, Xp, μf′;\n",
    "    linecolor=:blue,\n",
    "    linewidth=2.0,\n",
    "    label=\"Latent Function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the posterior of the first noise process to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter!(posterior_plot, X₁, ŷ₁;\n",
    "    markercolor=:red,\n",
    "    markershape=:circle,\n",
    "    markerstrokewidth=0.0,\n",
    "    markersize=4,\n",
    "    markeralpha=0.8,\n",
    "    label=\"Sensor 1\");\n",
    "scatter!(posterior_plot, X₂, ŷ₂;\n",
    "    markercolor=:green,\n",
    "    markershape=:circle,\n",
    "    markerstrokewidth=0.0,\n",
    "    markersize=4,\n",
    "    markeralpha=0.8,\n",
    "    label=\"Sensor 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the posterior plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(posterior_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What would change if we were to assume a different probability distribution over the noise in above model?\n",
    "- Redefine the model as a probabilistic model using [Gen](https://github.com/probcomp/Gen)\n",
    "          Hint: Have a look at the `Bayesian Linear Regression` example in the `Regression` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
