{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Regression with Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces you to regression analysis. Regression analysis estimates the relationship between the outcome variable and one/multiple independent variables. It splits into linear and logistic regression and is most commonly employed for prediction and forecasting.\n",
    "\n",
    "This tutorial is based on an amalgamation of [Flux's regression tutorial](https://github.com/FluxML/model-zoo/tree/master/other/iris) and [Gen's development branch treatise](https://probcomp.github.io/Gen/dev/getting_started/#Example-1) on Bayesian linear regression as an introductory topic for beginners in probabilistic programming.\n",
    "\n",
    "Structure:\n",
    "    1. Linear regression\n",
    "    2. Logistic regression\n",
    "    3. Exercise\n",
    "    4. Bayesian linear regression using probabilistic programming\n",
    "    5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the plotting package and Flux with its throttle function `Flux.throttle(f, timeout)`, which lets us use the `callback` function only every `timeout` seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using Flux\n",
    "using Flux: throttle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an artificial dataset with random disturbances in the range from $[-3, 3]$ for all linear regression experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an artificial dataset\n",
    "regX = randn(100)\n",
    "regY = 50 .+ 100 * regX + 20 * randn(100);\n",
    "\n",
    "# Plot the artificial dataset\n",
    "scatter(regX, regY, fmt = :png, legend=:bottomright, label=\"Artificial Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exact regression using linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the regression equations\n",
    "\n",
    "$ Y_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i}$\n",
    "\n",
    "regression can be expressed as a matrix equation\n",
    "\n",
    "$ Y = X \\beta + \\epsilon$\n",
    "\n",
    "which we can easily express for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with internal algebra system\n",
    "X = hcat(ones(length(regX)), regX)\n",
    "Y = regY\n",
    "intercept, slope = inv(X'*X) * (X'*Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results for later comparison to other approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regression line\n",
    "plot!((x) -> intercept + slope * x, -3, 3, label=\"fit_exact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Linear Regression using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the machine learning library [Flux](https://github.com/fluxml/flux.jl) we now express regression as a model of a single dense layer, which we train with gradient descent against the mean-squared error. Using Flux's `train!` macro we then combine these subparts into a single training routine for we use a `callback` routine to print the loss at every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data tuples\n",
    "data = zip(regX, regY)\n",
    "\n",
    "# Define the model\n",
    "model = Dense(1, 1, identity)\n",
    "\n",
    "# Mean-squared error\n",
    "loss(x, y) = Flux.mse(model([x]), y)\n",
    "\n",
    "# Callback function\n",
    "evalcb = () -> @show(sum([loss(i[1], i[2]) for i in data]))\n",
    "\n",
    "# Training with gradient descent\n",
    "opt = Descent(0.1)\n",
    "\n",
    "# Train for 50 epochs\n",
    "for i = 1:50\n",
    "    Flux.train!(loss, params(model), data, opt, cb=throttle(evalcb, 10))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the $\\theta$ and the bias from the trained model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(θ, bias) = collect(params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trained model against the previously found 'exact' linear algebra-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line trained with Flux\n",
    "plot!((x) -> bias[1] + θ[1] * x, -3, 3, label=\"fit_flux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression uses a logistic function to model datasets, for which we have binary measurements. The logistic function of x is:\n",
    "\n",
    "$1/(1 + \\exp^x)$\n",
    "\n",
    "It assigns probabilities to each output, which is then most commonly scored with the `crossentropy` loss function.\n",
    "Switching to logistic regression, we now use the `Iris` dataset, which is a classical pattern recognition dataset consisting of 3 classes with 50 instances each. Each class represents a type of iris plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: crossentropy, normalise, onecold, onehotbatch\n",
    "using Statistics: mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels & features of the Iris dataset\n",
    "labels = Flux.Data.Iris.labels()\n",
    "features = Flux.Data.Iris.features();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot encoding, encodes the categories of the dataset into an array, which is easier to digest for the machine as it contains binary values for the categories, i.e. 0s for not being part of the category and 1s for being a member of that category. For more explanation, please have a look at [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "normed_features = normalise(features, dims=2)\n",
    "\n",
    "# Split into classes and add one hot labels\n",
    "klasses = sort(unique(labels))\n",
    "onehot_labels = onehotbatch(labels, klasses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train on our dataset, we now have to split the data into a training- and test dataset. A quite common split would be 70% training data and to hold out 30% test data to later evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into trainings- and test-set\n",
    "train_indices = [1:3:150; 2:3:150]\n",
    "\n",
    "X_train = normed_features[:, train_indices]\n",
    "y_train = onehot_labels[:, train_indices]\n",
    "\n",
    "X_test = normed_features[:, 3:3:150]\n",
    "y_test = onehot_labels[:, 3:3:150];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the neural network in Flux with a single dense, fully-connected layer and subsequent softmax activation function to obtain probabilities as outputs. The `crossentropy`-loss measures the binary classification performance on a log loss scale skewed to high errors for weak predictive performance. For further explanation have a look at the [ML-cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example model with 4 features and 3 probabilities as outputs\n",
    "model = Chain(\n",
    "    Dense(4, 3),\n",
    "    softmax\n",
    ")\n",
    "\n",
    "loss(x, y) = crossentropy(model(x), y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient descent to optimise the neural network for the `crossentropy` loss defined above. The `train!` function takes the parameters of the `model`, our `crossentropy`-loss function, the data iterator and our gradient-`descent` optimiser and runs for the maximum number of iterations, as defined in the data iterator, i.e. `200` iterations in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "optimiser = Descent(0.5)\n",
    "data_iterator = Iterators.repeated((X_train, y_train), 200)\n",
    "\n",
    "Flux.train!(loss, params(model), data_iterator, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of trained model\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "accuracy_score = accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our performance on the dataset we now calculate the `confusion_matrix`, which helps us to differentiate between statistical 'true positive' and 'false positive'. The smaller the 'false positives' and 'false negatives' are, the better our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the confusion matrix for the model\n",
    "function confusion_matrix(X, y)\n",
    "    y_hat= onehotbatch(onecold(model(X)), 1:3)\n",
    "    y * y_hat'\n",
    "end\n",
    "\n",
    "display(confusion_matrix(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform linear or logistic regression on a dataset from [RDatasets.jl](https://github.com/JuliaStats/RDatasets.jl). You can query the available datasets with the following commands:\n",
    "    - `RDatasets.packages()`\n",
    "    - `RDatasets.datasets()`\n",
    "    - and e.g. `RDatasets.datasets(\"plm\")`, which would be good option for a linear regression model\n",
    "- Play around with the conceptual framework of linear and logistic regression by testing out different network types to query their behavior when used (e..g change in training time, accuracy etc.). The types of layers available in Flux are:\n",
    "    - Convolutional layers, `Conv(size, in=>out)`, and `Conv(size, in=>out, relu)`\n",
    "    - Recurrent layers, `RNN(in, out)`, `LSTM(in, out)`, and `GRU(in, out)`\n",
    "    - Dropout layers, `Dropout(p, dims)`, and `AlphaDropout(p)`. Dropout layers reduce overfitting by randomly dropping nodes of the network with probability `p`, to force the network to continuously adopt to this noisy training and not completely overfit to the one case at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bayesian linear regression through probabilistic programming (Outlook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Bayesian models efficiently, we have to rely on probabilistic programming languages, such as `Gen` and `Turing`, which seamlessly interface with the rest of the Julia ecosystem. This explicitly includes Flux. At its core these probabilistic programming systems construct a domain-specific language to express express probabilistic models, sample from probability distributions and utilize their integrated inference engines such as `Hamiltonian Monte-Carlo`, `Sequential Monte-Carlo`, `Variational Inference`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic programming works at its core with the abstract of a 'generative model'. Generative models describe a model defined in the Julia programming language and with `@gen` defined in the dynamic domain-specific language of Gen (to be explained in the afternoon)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generative model, which Gen relies on\n",
    "@gen function regression_model(regX::Vector{Float64})\n",
    "    slope = @trace(normal(110, 10), :slope)\n",
    "    intercept = @trace(normal(50, 2), :intercept)\n",
    "    for (i, x) in enumerate(regX)\n",
    "        @trace(normal(slope * x + intercept, 1), \"y-$i\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct an inference program, which takes in the data set and runs an inference routine on our model.\n",
    "For this we rely on the standard inference library at the core of Gen. This allows the scientist a higher-level\n",
    "abstraction, hence making the implementation of ideas and inference routines a lot easier and more accessible to\n",
    "non-experts. The inference program automatically fits 'slope' and 'intercept'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function regression_inference_program(regX::Vector{Float64}, regY::Vector{Float64}, num_iters::Int)\n",
    "    # Create a choicemap to constrain the set of possible y coordinates\n",
    "    # to the observed y values\n",
    "    constraints = choicemap()\n",
    "    for (i, y) in enumerate(regY)\n",
    "        constraints[\"y-$i\"] = y\n",
    "    end\n",
    "    \n",
    "    # Generate an initial execution trace in which the execution is constrained by\n",
    "    # the pre-defined 'constraints'\n",
    "    (trace, _) = generate(regression_model, (regX,), constraints)\n",
    "    \n",
    "    # Iteratively update the slope and the intercept using the\n",
    "    # in-built metropolis_hastings sampler\n",
    "    for iter=1:num_iters\n",
    "        (trace, _) = metropolis_hastings(trace, select(:slope))\n",
    "        (trace, _) = metropolis_hastings(trace, select(:intercept))\n",
    "    end\n",
    "    \n",
    "    # Retrieve 'slope' and 'intercept' values from final trace\n",
    "    choices = get_choices(trace)\n",
    "    return (choices[:slope], choices[:intercept])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference program\n",
    "(slope, intercept) = regression_inference_program(regX, regY, 10000)\n",
    "println(\"slope: $slope, intercept: $intercept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line trained with the inference program\n",
    "plot!((x) -> intercept + slope * x, -3, 3, label=\"bubu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think of a way to rewrite the logistic regression in a Bayesian style as done in 4.\n",
    "- Think of the above model:\n",
    "   - What influence do the priors have?\n",
    "   - Consider which probability distribution best represents the prior knowledge of model?\n",
    "   - Experiment with the probability distributions in Gen, the available distributions are:\n",
    "       - Bernoulli,            `bernoulli(prob_true::Real)`\n",
    "       - Beta,                `beta(alpha::Real, beta::Real)`\n",
    "       - Categorical,         `categorical(probs::Vector{Float64})`\n",
    "       - Exponential,         `exponential(rate::Real)`\n",
    "       - Gamma,               `gamma(shape::Real, scale::Real)`\n",
    "       - Geometric,           `geometric(p::Real)`\n",
    "       - Inverse Gamma,       `inv_gamma(shape::Real, scale::Real)`\n",
    "       - Multivariate Normal, `mvnormal(mu::AbstractVector{Float64}, cov::AbstractMatrix{Float64})`\n",
    "       - Poisson,             `poisson(lambda::Real)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
