{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to Differential Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the reader to the fundamental concepts of differential program through concepts such as the 'Neural ODE' and 'universal' differential equations. Differential programming aims at its core to better exploit inherent problem structure to allow for significant model simplifications. At the core of this endeavour are autodifferentiation toolkits to obtain the gradients of instances, as well as complete program executions. Given current state-of-the-art AD toolkits we can take derivatives over pretty much any function 'f(x)'. This is an especially enticing perspective given current frameworks' excessive computational needs to construct and train model, as they always start training from 0.\n",
    "\n",
    "Remark: The current frameworks are sadly not ready yet for large-scale deployments coupled with already validated scientific simulators.\n",
    "\n",
    " \n",
    " ## List of References:\n",
    " \n",
    " **Reference 1.** [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366)\n",
    " \n",
    " **Reference 2.** [Universal Differential Equations for Scientific Machine Learning](https://arxiv.org/abs/2001.04385)\n",
    " \n",
    " **Reference 3.** [What Is Differentiable Programming?](https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html)\n",
    " \n",
    " **Reference 4.** [Differentiable Control Problems](https://fluxml.ai/2019/03/05/dp-vs-rl.html)\n",
    " \n",
    " \n",
    " ## Outline:\n",
    " \n",
    " **Section 1.** [Neural Ordinary Differential Equations](#ode)\n",
    " \n",
    " **Section 2.** [Universal Differential Equations](#universal)\n",
    " \n",
    " **Section 3.** [Exercise - Implement your own Neural Differential Equation](#ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Neural Ordinary Differential Equations <a name=\"ode\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea of the neural ODE can best be described by parameterizing the derivative of the latent states using a neural network. One can then subsequently apply a standard ODE solver to solve for the equations, which allows us to explicitly tradeoff between computation speed and accuracy. This is given in the following algorithm:\n",
    "\n",
    "<img src=\"imgs/NeuralODE.png\" width=\"750\" height=\"350\" />\n",
    "\n",
    "(Source: Neural Ordinary Differential Equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DifferentialEquations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the Lotka-Volterra equation, which is most commonly known for modelling the relative population of predator-prey biological dynamical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra(du, u, p, t)\n",
    "    x, y = u\n",
    "    alpha, beta, delta, gamma = p\n",
    "    du[1] = dx = alpha * x - beta * x * y\n",
    "    du[2] = dy = -delta * y + gamma * x * y\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the artificial initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = [1.0, 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5, 1.0, 3.0, 1.0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the ODE problem with DiffEq.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = ODEProblem(lotka_volterra, u0, tspan, p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the ODE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: solve not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: solve not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[1]:1"
     ]
    }
   ],
   "source": [
    "sol = solve(prob);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the initial results with Plots.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "plot(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing the inputs to the Lotka-Volterra equation as functions we can now generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0_f(p, t0) = [p[2], p[4]];\n",
    "tspan_f(p) = (0.0, 10 * p[4]);\n",
    "p = [1.5, 1.0, 3.0, 1.0];\n",
    "prob = ODEProblem(lotka_volterra, u0_f, tspan_f, p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which now gives us the opportunity to first solve this ODE classically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [1.5, 1.0, 3.0, 1.0];\n",
    "prob = ODEProblem(lotka_volterra, u0, tspan, p);\n",
    "sol = solve(prob, Tsit5(), saveat=0.1);\n",
    "A = sol[1, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "plot(sol)\n",
    "t = 0:0.1:10.0\n",
    "scatter!(t, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then with the more 'modern' interface of Flux.jl and DiffEqFlux.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Flux and DiffEqFlux\n",
    "using Flux, DiffEqFlux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve using the interface of DiffEqFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_solve(prob, Tsit5(), u0, p, saveat=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function can now be used as the input to a neural network, where we use the ODE in the predictive part of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [2.2, 1.0, 2.0, 0.4]\n",
    "params = Flux.params(p)\n",
    "\n",
    "function predict_rd()\n",
    "    concrete_solve(prob, Tsit5(), u0, p, saveat=0.1)[1, :]\n",
    "end\n",
    "\n",
    "loss_rd() = sum(abs2, x-1 for x in predict_rd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For which we can now employ the whole machinery of Flux.jl with an ADAM optimizer to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Iterators.repeated((), 100)\n",
    "opt = ADAM(0.1)\n",
    "cb = function ()\n",
    "    display(loss_rd())\n",
    "    # update parameter p with remake\n",
    "    display(plot(solve(remake(prob, p=p), Tsit5(), saveat=0.1), ylim=(0, 6)))\n",
    "end\n",
    "\n",
    "# Display ODE\n",
    "cb()\n",
    "\n",
    "Flux.train!(loss_rd, params, data, opt, cb = cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A third option is to embed the ODE directly into the neural network, as done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the ODE into a multilayer perceptron\n",
    "m = Chain(\n",
    "    Dense(28^2, 32, relu),\n",
    "    # An ODE of 32 parameters\n",
    "    p -> concrete_solve(prob, Tsit5(), u0, p, saveat=0.1)[1,:],\n",
    "    Dense(32, 10),\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overarching requirement across all these different approaches to define and/or solve an ODE is the fact that we must always be able to define the forward pass of the solver. This is valid for both, machine learning and scientific computing. Validating against the implementation from the NeuralODE paper.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the example from the Neural ODE release paper\n",
    "u0 = Float32[2.; 0.]\n",
    "datasize = 30\n",
    "tspan = (0.0f0, 1.5f0)\n",
    "\n",
    "function trueODEfunc(du, u, p, t)\n",
    "    true_A = [-0.1 2.0; -2.0 -0.1]\n",
    "    du .= ((u.^3)'true_A)'\n",
    "end\n",
    "\n",
    "t = range(tspan[1], tspan[2], length=datasize)\n",
    "prob = ODEProblem(trueODEfunc, u0, tspan)\n",
    "ode_data = Array(solve(prob, Tsit5(), saveat=t));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the forward pass using the higher-level abstraction of a 'NeuralODE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass\n",
    "dudt = Chain(x -> x.^3,\n",
    "             Dense(2, 50, tanh),\n",
    "             Dense(50, 2))\n",
    "ps = Flux.params(dudt)\n",
    "n_ode = NeuralODE(dudt, tspan, Tsit5(), saveat=t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prediction function for the neural network and a classical loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict_n_ode()\n",
    "    n_ode(u0)\n",
    "end\n",
    "\n",
    "loss_n_ode() = sum(abs2, ode_data .- predict_n_ode());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the ODE like we would normally train a neural network, including callback function and '@train' macro. The plot will be shown multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "data = Iterators.repeated((), 1000)\n",
    "opt = ADAM(0.1)\n",
    "cb = function ()\n",
    "    display(loss_n_ode())\n",
    "    # plot current prediction against data\n",
    "    cur_pred = predict_n_ode()\n",
    "    pl = scatter(t, ode_data[1, :], label=\"data\")\n",
    "    scatter!(pl, t, cur_pred[1, :], label=\"prediction\")\n",
    "    display(plot(pl))\n",
    "end\n",
    "\n",
    "# Display the ODE with the initial parameter values\n",
    "cb()\n",
    "\n",
    "ps = Flux.params(n_ode)\n",
    "Flux.train!(loss_n_ode, ps, data, opt, cb=cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Universal Differential Equations <a name=\"universal\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working from the realization that a neural network layer, be it with an embedded ODE or not, is always just a differentiable function a successor to the Neural ODEs was developed called the 'universal differential equations'. Possible types of universal differential equations, which we can implement are:\n",
    "\n",
    "- Universal Ordinary Differential Equations (UODEs)\n",
    "- Universal Stochastic Differential Equations (USDEs)\n",
    "- Universal Delay Differential Equations (UDDEs)\n",
    "- Universal Differential-Algebraic Equations (UDAEs)\n",
    "- Universal Boundary Value Problems (UBVPs)\n",
    "- Universal Partial Differential Equations (UPDEs)\n",
    "- Universal Hybrid (Event-Driven) Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Solving with adjoints - Partial Neural Adjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, Flux, OrdinaryDiffEq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = Float32[0.8; 0.8]\n",
    "tspan = (0.0f0, 25.0f0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the neural network and retrieve its parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Chain(Dense(2, 10, tanh), Dense(10,1))\n",
    "\n",
    "p1, re = Flux.destructure(ann)\n",
    "p2 = Float32[-2.0, 1.1]\n",
    "p3 = [p1; p2]\n",
    "ps = Flux.params(p3, u0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the forward pass and solve the ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dudt_(du, u, p, t)\n",
    "    x, y = u\n",
    "    du[1] = re(p[1:41])(u)[1]\n",
    "    du[2] = p[end-1] * y + p[end] * x\n",
    "end\n",
    "\n",
    "prob = ODEProblem(dudt_, u0, tspan, p3)\n",
    "concrete_solve(prob, Tsit5(), u0, p3, abstol=1e-8, reltol=1e-6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the adjoint to obtain the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict_adjoint()\n",
    "    Array(concrete_solve(prob, Tsit5(), u0, p3, saveat=0.0:0.1:25.0, abstol=1e-8, reltol=1e-6))\n",
    "end\n",
    "\n",
    "loss_adjoint() = sum(abs2, x-1 for x in predict_adjoint());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the neural network and have a look at the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Iterators.repeated((), 100)\n",
    "opt = ADAM(0.1)\n",
    "cb = function ()\n",
    "    display(loss_adjoint())\n",
    "end\n",
    "\n",
    "# Display ODE\n",
    "cb()\n",
    "\n",
    "Flux.train!(loss_adjoint, ps, data, opt, cb = cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Solving with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DiffEqFlux, Flux, OrdinaryDiffEq, Optim, Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u0 = Float32[0.8, 0.8]\n",
    "tspan = (0.0f0, 25.0f0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Chain(Dense(2, 10, tanh), Dense(10, 1))\n",
    "\n",
    "p1, re = Flux.destructure(ann)\n",
    "p2 = Float32[0.5, -0.5]\n",
    "p3 = [p1; p2]\n",
    "ptrain = [p3; u0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the forward pass and the ODE solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass\n",
    "function dudt_(du, u, p, t)\n",
    "    x, y = u\n",
    "    du[1] = re(p[1:41])(u)[1]\n",
    "    du[2] = p[end-1] * y + p[end] * x\n",
    "end\n",
    "\n",
    "prob = ODEProblem(dudt_, u0, tspan, p3)\n",
    "concrete_solve(prob, Tsit5(), u0, p3, abstol=1e-8, reltol=1e-6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the adjoint, including gradients taken with Zygote for a faster convergence of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the adjoint\n",
    "function predict_adjoint(fullp)\n",
    "    Array(concrete_solve(prob, Tsit5(), fullp[end-1:end], fullp[1:end-1], saveat=0.0:0.1:25.0, abstol=1e-8, reltol=1e-6))\n",
    "end\n",
    "\n",
    "loss_adjoint(fullp) = sum(abs2, x-1 for x in predict_adjoint(fullp))\n",
    "\n",
    "function loss_adjoint_gradient!(G, fullp)\n",
    "    G .= Zygote.gradient(loss_adjoint, fullp)[1]\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using BFGS including first-order gradients of the adjoint taken with Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(loss_adjoint, loss_adjoint_gradient!, ptrain, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Exercise - Implement your own Neural Differential Equation <a name=\"ex\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement your own differential equation.\n",
    "    - Experiment with the performance between the optimization-based BFGS algorithm and the neural adjoint on your\n",
    "        self-set problem.\n",
    "- Construct universal differential equations on the basis of recurrent neural networks and convolutional neural networks.\n",
    "- Implement the Korteweg-de Vries (KdV) equation and examine the framework's behavior when dealing with high-order derivatives.\n",
    "- Looking at the [Trebuchet example](https://github.com/FluxML/model-zoo/tree/cdda5cad3e87b216fa67069a5ca84a3016f2a604/games/differentiable-programming/trebuchet) for control policies in differentiable programming, develop a control policy for the above two equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
